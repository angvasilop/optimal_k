%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Angelos at 2023-06-06 21:31:52 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@misc{Bates2022,
	archiveprefix = {arXiv},
	author = {Stephen Bates and Trevor Hastie and Robert Tibshirani},
	date-added = {2023-06-06 21:31:34 -0500},
	date-modified = {2023-06-06 21:31:52 -0500},
	eprint = {2104.00673},
	month = {07},
	primaryclass = {stat.ME},
	title = {Cross-validation: what does it estimate and how well does it do it?},
	year = {2022}}

@book{James2021,
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	date-added = {2023-05-23 14:42:11 -0500},
	date-modified = {2023-05-24 17:43:57 -0500},
	edition = {2},
	month = {07},
	pages = {426},
	publisher = {Springer},
	title = {An Introduction to Statistical Learning},
	year = {2021}}

@article{Marcot2021,
	author = {Marcot, Bruce and Hanea, Anca},
	date-added = {2023-05-10 16:50:38 -0500},
	date-modified = {2023-05-24 17:34:55 -0500},
	journal = {Computational Statistics},
	month = {09},
	pages = {2009-2031},
	title = {What is an optimal value of k in k-fold cross-validation in discrete Bayesian network analysis?},
	volume = {36},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s00180-020-00999-9}}

@article{Shao1993,
	abstract = {We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular leave-
one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaike information criterion (AIC), the Cp, and the bootstrap, is asymptotically in the sense that the probability inconsistent of selecting the model with the best predictive ability does not converge to 1 as the total number of observations n -s o. We show that the of the leave-one-out cross-validation inconsistency can be rectified by using a leave-n,-out with nv, the number of cross-validation observations reserved for validation, satisfying no/n -1 I as n s* xoo. This is a somewhat shocking discovery, because ne/n -* 1 is totally opposite to the popular leave-one-out recipe in cross-validation. and discussions of some practical Motivations, justifications, aspects of the use of the leave-n,-out cross-validation method are provided, and results from a simulation study are presented},
	author = {Shao, Jun},
	date-added = {2023-05-09 22:34:38 -0500},
	date-modified = {2023-05-24 16:20:58 -0500},
	journal = {Journal of the American Statistical Association},
	month = {02},
	pages = {486-494},
	title = {Linear model selection by cross-validation},
	volume = 88,
	year = 1993,
	bdsk-url-1 = {https://doi.org/10.1080/01621459.1993.10476299}}

@article{Yang2006,
	author = {Yang, Yuhong},
	date-added = {2023-05-09 22:28:05 -0500},
	date-modified = {2023-05-24 16:17:45 -0500},
	journal = {Statistica Sinica},
	month = {04},
	pages = {635-657},
	title = {Comparing Learning Methods for Classification},
	volume = {16},
	year = {2006}}

@article{Kohavi2001,
	author = {Kohavi, Ron},
	date-added = {2023-05-06 17:25:51 -0500},
	date-modified = {2023-05-24 16:00:55 -0500},
	journal = {International Joint Conference on Artificial Intelligence},
	month = {03},
	read = {0},
	title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
	volume = {14},
	year = {2001}}

@article{Bengio2004,
	author = {Bengio, Yoshua and Grandvalet, Yves},
	date-added = {2023-05-06 17:22:50 -0500},
	date-modified = {2023-05-24 15:57:32 -0500},
	journal = {Journal of Machine Learning Research},
	month = {06},
	pages = {1089-1105},
	title = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
	volume = {5},
	year = {2004},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAyLi4vLi4vLi4vLi4vb3B0aW1hbF9rX2dpdC9hcnRpY2xlcy9iZW5naW9fMjAwNC5wZGZPEQFmAAAAAAFmAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADghCfwQkQAAf////8PYmVuZ2lvXzIwMDQucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////+BwcYAAAAAAAAAAAAAEAAMAAAogY3UAAAAAAAAAAAAAAAAACGFydGljbGVzAAIANi86VXNlcnM6YW5nZWxvczpvcHRpbWFsX2tfZ2l0OmFydGljbGVzOmJlbmdpb18yMDA0LnBkZgAOACAADwBiAGUAbgBnAGkAbwBfADIAMAAwADQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASADRVc2Vycy9hbmdlbG9zL29wdGltYWxfa19naXQvYXJ0aWNsZXMvYmVuZ2lvXzIwMDQucGRmABMAAS8AABUAAgAO//8AAAAIAA0AGgAkAFkAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABww==}}

@article{Burman1989,
	author = {Burman, Prabir},
	date-added = {2023-05-06 17:21:07 -0500},
	date-modified = {2023-05-24 17:46:32 -0500},
	journal = {Biometrika},
	month = {09},
	pages = {503-514},
	title = {A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods},
	volume = {76},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1093/biomet/76.3.503}}

@article{Breiman1992,
	author = {Breiman, Leo and Spector, Philip},
	date-added = {2023-05-06 17:09:06 -0500},
	date-modified = {2023-05-24 17:33:52 -0500},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	month = {12},
	pages = {291},
	title = {Submodel selection and evaluation in regression. The X-Random Case},
	volume = {60},
	year = {1992}}

@article{Efron1983,
	author = {Efron, Bradley},
	date-added = {2023-05-06 17:09:06 -0500},
	date-modified = {2023-05-24 16:20:02 -0500},
	journal = {Journal of the American Statistical Association},
	month = {03},
	pages = {316-331},
	title = {Estimating the error rate of a prediction rule: Improvement on cross-validation},
	volume = {78},
	year = {1983}}

@article{Zhang2015,
	author = {Zhang, Yongli and Yang, Yuhong},
	date-added = {2023-05-06 17:09:06 -0500},
	date-modified = {2023-05-24 16:19:26 -0500},
	journal = {Journal of Econometrics},
	month = {02},
	pages = {95-112},
	title = {Cross-validation for selecting a model selection procedure},
	volume = {187},
	year = {2015}}

@misc{Austern2020,
	archiveprefix = {arXiv},
	author = {Morgane Austern and Wenda Zhou},
	date-added = {2023-05-06 17:09:06 -0500},
	date-modified = {2023-05-24 17:41:38 -0500},
	eprint = {2001.11111},
	month = {01},
	title = {Asymptotics of Cross-Validation},
	year = {2020}}

@inproceedings{Kumar2013,
	author = {Kumar, Ravi and Lokshtanov, Daniel and Vassilvitskii, Sergei and Vattani, Andrea},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	date-added = {2023-05-06 17:09:06 -0500},
	date-modified = {2023-05-24 17:36:00 -0500},
	month = {06},
	pages = {27-35},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Near-Optimal Bounds for Cross-Validation via Loss Stability},
	volume = {28},
	year = {2013},
	bdsk-url-1 = {https://proceedings.mlr.press/v28/kumar13a.html}}

@inproceedings{Kale2011,
	author = {Kale, Satyen and Kumar, Ravi and Vassilvitskii, Sergei},
	booktitle = {International Conference on Supercomputing},
	date-added = {2023-05-06 17:09:06 -0500},
	date-modified = {2023-05-24 17:38:22 -0500},
	month = {01},
	pages = {487-495},
	title = {Cross-Validation and Mean-Square Stability},
	year = {2011}}
