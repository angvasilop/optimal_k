---
title: "test"
author:
- Angelos Vasilopoulos
- Gregory J. Matthews
abstract: "| Abstract \\vspace{2mm}\n| *Keywords*: Cross validation  \n"
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    keep_tex: yes
  word_document: default
fontsize: 12pt
link-citations: yes
linkcolor: cyan
urlcolor: cyan
bibliography: /Users/angelos/Documents/research/optimal_k/optimal_k_references.bib
header-includes: \usepackage{setspace} \setstretch{1.15} \usepackage{float} \floatplacement{figure}{t}
indent: TRUE
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE
)
gjm <- function(x, color = "red") {
  sprintf("\\textcolor{%s}{%s}", color, x)}
```

\newpage

# Introduction {#sec:intro}

$k$-fold cross-validation is a popular method of error estimation for model selection in computational research.

n observations are divided into $k$ groups. In a first iteration, $i = k - 1$ groups are used as a training set. The remaining group is used as a test set to estimate prediction error. This process is repeated $i$ times, with a different test set in each iteration. The average of $k$ prediction errors ($PE$)
$$\hat{\theta} = CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k}PE_i$$

is meant to estimate the true model error $\theta$, i.e., the error of the model tested on the population.

Despite the popularity of cross-validation, there is limited focus in the literature on the question of what fold number $k$ is appropriate for cross-validation with a given data set. One popular idea is that selection of $k$ comes with a bias-variance trade-off, specifically, that as $k$ increases, the bias of k-fold error estimation
$$Bias(\hat{\theta}) = E(\hat{\theta}) - \theta$$
and variance of $k$-fold error estimation
$$Var(\hat{\theta}) = E(\hat{\theta}^2) - E(\hat{\theta})^2$$
as in
$$Err(\hat{\theta}) = Bias^2(\hat{\theta}) + Var(\hat{\theta})$$
decrease and increase, respectively. This idea appears in early literature (@Efron1983; @Kohavi2001).

Subsequent, albeit limited, literature argues differently. In the case of leave-one-out cross-validation, i.e., with $k$ = 1, previous literature suggests that asymptotically both bias and variance of error estimation decrease as $k$ increases (@Burman1989) and that bias and variance of error estimation are uniformly low (@Breiman1992).

More recently have been proposed ways to quantify the variance reduction achieved by cross-validation when the true prediction error is not known, e.g., as mean-square stability (@Kale2011), which measures performance estimate variance across folds, or as loss stability (@Kumar2013), the proportion of times that a model outperforms competing models in all folds. However, it has also been demonstrated that, due to overlap between training and test sets in cross-validation, there is no universal (i.e., valid under all distributions) unbiased estimator of the variance of $k$-fold cross validation (@Bengio2004).

In addition to theoretical analysis of variance reduction by cross validation, there are some simulation results showing this phenomenon (@Zhang2015). However, simulations currently in the literature provide limited insight into the dependence of optimal fold number $k$ on sample size $n$. Here we present a simulation of linear regression and least absolute shrinkage and regression operator (LASSO) regression to observe the relationship of cross-validation fold number $k$ and model prediction error estimation accuracy for various sample sizes $n$.

# Simulation

Consider a population of size $N = 500,000$ with five of 100 features $X \sim N(0, 1)$ a linear combination
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \epsilon$$
where $\beta_1...\beta_5 = 1$ and $\epsilon \sim N(0, 10)$. Then the true model of the population is
$$E(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5$$
and the mean squared error ($MSE$) of the true model is
$$MSE[E(Y)] = \theta = \frac{\sum_{i=1}^{N}[Y - E(Y)]^2}{N}.$$

From the population we take a sample of size $n$. We estimate $Y$ as $\hat{Y}$ by regression on a subset of $X$ and estimate $MSE(\hat{Y})$ as 
$$\hat{\theta} = \frac{\sum_{i=1}^{N}(Y - \hat{Y})^2}{N}$$
by $k$-fold cross-validation. We repeat this procedure in 1000 simulations for the same subsets of $X$, including the five-variable subset of the true model, and values of $k$. We then calculate simulation-wise $MSE(\hat{\theta})$ for each value of $k$.

We perform an additional 1000 simulations predicting $Y$ as $\hat{Y}$ by LASSO regression instead of linear regression. To avoid data leakage, we introduce an inner 5-fold cross-validation loop for the selection of regularization parameter $\lambda$ as in the minimization of
$$\sum_{i=1}^{n}(Y_i - \sum_{j}X_{ij}\beta_j)^2 + \lambda\sum_{j=1}^{p}|\beta_j|.$$

We perform the same 1000 simulations with $n$ = {100, 500, 1000} and consider as the optimal value of $k$ the value $k_{optimal}$ resulting in the lowest $MSE(\hat{\theta})$ for the true model the greatest number of times.

This is because in practice the model with the lowest prediction error is more likely to be selected. We refer to this as lowest-error model selection. As our simulation results show, however, it is possible for $k$-fold cross validation to result in calculations of $MSE(\hat{\theta})$ that suggest that a competing model has a lower prediction error than the true model. This false model may have the lowest prediction error in $k$-fold cross validation but its generalization error will be lower in the long-run (i.e., when tested on a large part of the population) than the generalization error of the true model, as $E(Y)$ is a linear combination of the true model only. Thus, it is preferable for the value of $k$ selected to result in the true model having the lowest estimated prediction error.

# Results

We find that for all $n$ as fold number $k$ increases, both bias and variance of error estimation decrease initially before leveling off (Figures 1 - 3).
```{r echo=FALSE}
source("~/Documents/research/optimal_k/results/optimal_k_plots.R")
figure1
figure2
figure3
```
We also find that for larger $n$, e.g., $n = 1000$, lowest-error model selection becomes less reliable as $k$ increases past a certain value, i.e., as $k$ increases past a certain value the true model has the lowest MSE less frequently. With increasing $n$ we also observe variability in the improvement of lowest-error model selection reliability (Figure 4; Table 1).
```{r echo=FALSE}
figure4
```
```{r}
kable_styling(kable(dffortable), latex_options = "hold_position")
```

# Conclusion

Early literature suggests that increasing cross-validation fold number is related to decreasing bias and increasing variance of error estimation. However, more recent work suggests that this is not the case. Instead, increasing $k$ results in bias and variance reduction. This phenomenon is observable in our simulation results, although it is also clear from the results that the reductions are not limitless, meaning that there is a point of diminishing returns in increasing $k$ and computational power allocation, as error estimation does not improve after a certain value of $k$.

Our results also indicate a relationship between optimal fold number $k$ and sample size $n$, although more data is necessary to elucidate a trend. This is interesting as a future direction, as modeling the relationship between $n$ and $k_{optimal}$ would have practical utility, potentially improving the selection of $k$ from a largely arbitrary decision between 5 and 10. Future research may also focus on error estimation of other models, including models capturing non-linear relationships or involving tuning of multiple hyper-parameters, e.g., random forest or gradient boosting.

# Acknowledgements {.unnumbered}

Dr. Gregory Matthews's Fall 2022 Predictive Analytics class.

# Supplementary Material {.unnumbered}

All code for reproducing the analyses in this paper is publicly available at <https://github.com/gjm112/optimal_k>.

# References


