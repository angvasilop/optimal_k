---
title: "Simulations of fold number selection in cross-validation of linear and LASSO regression"
author:
- Angelos Vasilopoulos
- Gregory J. Matthews
abstract: "*k*-fold cross-validation is a popular method of error estimation for model selection in computational research. However, there is limited focus in the literature on the question of what fold number *k* is appropriate for various dataset dimensions. Here we review relevant literature and present a simulation of linear and least absolute shrinkage and selection operator (LASSO) regression prediction error estimation at various values of *k* and sample size *n*. In agreement with a growing body of literature, we find that contrary to a persisting understanding, there is no bias-variance trade-off in selection of *k*. Instead, with increasing *k* both bias and variance decrease, perhaps asymptotically. Our results also suggest a predictable relationship between optimal values of *k* and *n*. | *Keywords*: cross-validation, optimization, fold number"
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    keep_tex: yes
  word_document: default
fontsize: 12pt
link-citations: yes
linkcolor: cyan
urlcolor: cyan
bibliography: /Users/angelos/Documents/research/optimal_k/manuscript/optimal_k_references.bib
header-includes: \usepackage{setspace} \setstretch{1.15} \usepackage{float} \floatplacement{figure}{t} \usepackage[labelsep=period]{caption}
indent: TRUE
---

```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	fig.pos = 'H'
)
gjm <- function(x, color = "red") {
  sprintf("\\textcolor{%s}{%s}", color, x)}
```

# Introduction {#sec:intro}

$k$-fold cross-validation is a popular method of error estimation for model selection in computational research. $n$ observations are divided into $k$ groups. In a first iteration, $i = k - 1$ groups are used as a training set. The remaining group is used as a test set to calculate estimated prediction error ($\mathrm{\widehat{PE}}$). This process is repeated $i$ times, with a different test set in each iteration. The average of $k$ estimates of $\mathrm{PE}$
$$\hat{\theta} = CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k}\mathrm{\widehat{PE}}_i$$
is meant to estimate the true model error $\theta$, i.e., the error of the model tested on the population (@Bates2022).

Despite the popularity of cross-validation, there is limited focus in the literature on the question of what fold number $k$ is appropriate for cross-validation with a dataset of a given size. One popular idea is that selection of $k$ comes with a bias-variance trade-offâ€”specifically, that as $k$ increases, the bias $Bias(\hat{\theta}) = \mathrm{E}(\hat{\theta}) - \theta$ and variance $Var(\hat{\theta}) = \mathrm{E}(\hat{\theta}^2) - \mathrm{E}(\hat{\theta})^2$ of $k$-fold error estimation decrease and increase, respectively. This idea appears in early literature (@Efron1983; @Kohavi2001) but also in modern, widely used textbooks (@James2021).

Subsequent, albeit limited, literature argues differently. In the case of leave-one-out cross-validation (LOOCV), i.e., with $k = n$, some authors suggest that asymptotically both bias and variance of error estimation decrease as $k$ increases (@Burman1989) and that bias and variance of error estimation are uniformly low (@Breiman1992).

More recently have been proposed ways to quantify the variance reduction achieved by cross-validation when the true prediction error ($\mathrm{PE}$) is not known, e.g., as mean-square stability (@Kale2011) or as loss stability (@Kumar2013). However, it has also been demonstrated that, due to overlap between training and test sets in cross-validation, there is no universal (i.e., valid under all distributions) unbiased estimator of the variance of $k$-fold cross-validation (@Bengio2004).

In addition to theoretical analyses of variance reduction by cross-validation, there are some simulation results showing this phenomenon. However, simulations currently in the literature provide limited insight into the dependence of optimal fold number $k_\mathrm{optimal}$ on sample size $n$ (@Zhang2015) or involve biased variance calculations (@Marcot2021). Here we present a simulation of linear regression and least absolute shrinkage and selection operator (LASSO) regression to observe the relationship of cross-validation fold number $k$ to model selection accuracy for various samples of size $n$ of a known population.

# Simulation

In machine learning tasks, selection of $k$ for $k$-fold cross validation is largely an arbitrary decision between, e.g., 5 and 10. With the results of the following simulation, we make a less arbitrary recommendation of $k$ with the aim of improving model selection accuracy.

Consider a population of size N = 500,000 with five of $P = 100$ features $X_1...X_{100} \sim N(0, 1)$ a linear combination of feature $Y = \beta_0 + \beta_1X_1 + \cdots + \beta_5X_5 + \epsilon$
where $\beta_1 = \cdots = \beta_5 = 1$ and $\epsilon \sim N(0, 10)$. The true model of $Y$ is $f \in F$, a set of competing models. $f = \mathrm{E}(Y) = \beta_0 + \beta_1X_1 + \cdots + \beta_5X_5$
and its mean squared error ($\mathrm{MSE}$) is
$$\theta = \mathrm{MSE}(f) = \frac{\sum_{i=1}^{N}[Y_i - \mathrm{E}(Y_i)]^2}{N}.$$

From the population we take a sample of size $n$. We estimate $Y$ as $\hat{Y}$ by regression on a subset of $X_1...X_{100}$, regression coefficients estimated by the least squares method, and compute $\mathrm{MSE}(\hat{Y})$ by $k$-fold cross-validation as
$$\hat{\theta} = \mathrm{MSE}(\hat{Y}) = \frac{1}{k}\sum_{j=1}^{k}\frac{\sum_{i=1}^{n/k}(Y_{j_i} - \hat{Y_{j_i}})^2}{n/k}$$
where $j_i$ is the index of the $i^{th}$ element of the $j^{th}$ fold.

In addition to regression with $X_1...X_5$, $f$, we under-fit with $X_1...X_3$, over-fit with $X_1...X_{20}$ and $X_1...X_{100}$, and do regression with noise $X_6...X_{100}$ only, for each $k \in$ {2, 10, 20, 30, ..., $n$} for each  $n \in$ {100, 200, 300, ..., 1000}. We perform this simulation 1000 times and then calculate simulation-wise $\mathrm{MSE}(\hat{\theta})$ for each $k$, for each $n$.

We perform an additional 1000 simulations predicting $Y$ as $\hat{Y}$ by LASSO regression instead of linear regression for each $k \in$ {2, 10, 20, 30, ..., 100, $n$} for each $n \in$ {250, 500, 750, 1000}. To avoid data leakage, we introduce an inner 5-fold cross-validation loop for the selection of parameter $\lambda$ as in the minimization of
$$\sum_{i=j}^{p}(Y_j - \beta_j)^2 + \lambda\sum_{j=1}^{p}|\beta_j|.$$

For each $k$, for each $n$, we count the number of times $A$ that the true model is selected and consider as optimal fold number for each $n$ the value $k_\mathrm{optimal}$, at which $A$ is the highest.

This is because in practice the model with the lowest $\mathrm{\widehat{PE}}$ is more likely to be selected. We refer to this as lowest-error model selection. As our simulation results show, however, it is possible for $k$-fold cross-validation to result in calculations of $\mathrm{MSE}(\hat{\theta})$ such that a competing model $\hat{f} \in F$ has a lower $\mathrm{\widehat{PE}}$ than the true model $f$. This false model may have the lowest $\mathrm{\widehat{PE}}$ but its generalization error will be higher in the long-run (i.e., when tested on a large part of the population) than the generalization error of the true model. Thus, it is preferable for the value of $k$ selected to result in $f$ having the lowest $\mathrm{\widehat{PE}}$.

Having considered the effect of $k$ on model selection accuracy for a given $n$, we may also consider the relationship of $k_\mathrm{optimal}$ and $n$. Our results suggest that after a certain value of $k$, changes in $A$ become negligible, in the case of linear regression, or negative, in the case of LASSO regression, and resource-expensive increases of fold number become undesirable. We refer to this "point of diminishing returns" as $k_\mathrm{optimal}^*$.

To estimate $k_\mathrm{optimal}*$ for each $n$, we first fit to the dataset $(k,A)$ for each sample size $n$ a model of the form
$$\hat{A}(k) = Bk + C + \frac{D}{k}$$
with constants $B$ to $D$ specific to each $n$. Certain more complicated models (e.g., with higher-order terms) may provide better approximation of $A$ but make optimization by the following scheme non-trivial. We draw a line $L$ through $(k_1, \hat{A}_1)$ and $(k_n, \hat{A}_n)$ and maximize the perpendicular distance $d$ of each point on $\hat{A}$ from $L$, so that
\begin{align*}
k^*_{\mathrm{optimal}} &= {\arg\max}_k{d[\hat{A}(k),L]} \\
                       &= {\arg\max}_k{d[Bk+C+\frac{D}{k},y-(Bn+C+\frac{D}{n})=m(x-n)]} \\
                       &= {\arg\max}_k{d[Bk+C+\frac{D}{k},y-(Bn+C+\frac{D}{n})=\frac{(Bn+C+\frac{D}{n})-(2B+C+\frac{D}{2})}{n-2}(x-n)]} \\
                       &= {\arg\max}_k{d[Bk+C+\frac{D}{k},y-(Bn+C+\frac{D}{n})=(B-\frac{D}{2n})(x-n)]} \\
                       &= {\arg\max}_k{d[Bk+C+\frac{D}{k},(B-\frac{D}{2n})x-y+C+\frac{D}{2}+\frac{D}{n}=0]}. \\
\end{align*}
It is known that the distance $d$ between a point $(k, \hat{A}(k))$ and a line of the form
$$ax + by + c = 0$$
is
\begin{align*}
d = \frac{|ak + b\hat{A}(k) + c|}{\sqrt{a^2+b^2}}
\end{align*}
so
\begin{align*}
k^*_{\mathrm{optimal}} &= {\arg\max}_k{d[Bk+C+\frac{D}{k},(B-\frac{D}{2n})x-y+C+\frac{D}{2}+\frac{D}{n}=0]} \\
                       &= {\arg\max}_k{\frac{|(B-\frac{D}{2n})k-(Bk+C+\frac{D}{k})+(C+\frac{D}{2}+\frac{D}{n})|}{\sqrt{(B-\frac{D}{2n})^2+(-1)^2}}}.
\end{align*}
Maximizing $d$, knowing that $n$ and $k$ are always positive, we find that
\begin{align*}
\frac{d}{dk}d[\hat{A}(k),L] &= \frac{d}{dk}\left(\frac{|(B-\frac{D}{2n})k-(Bk+C+\frac{D}{k})+(C+\frac{D}{2}+\frac{D}{n})|}{\sqrt{(B-\frac{D}{2n})^2+(-1)^2}}\right) \\
                            &=\frac{|n||k|(2nD-Dk^2)(-Dk^2-2nD+nDk+2Dk)}{nk^3|-Dk^2-2nD+nDk+2Dk|\sqrt{D^2+4n^2}} = 0 \\
                            &\Rightarrow k = 0, 2, \sqrt{2n}, n.
\end{align*}
Evaluating $d$ at these four values of $k$, we find that
\begin{align*}
d(0) &= \mathrm{DNE} \\
d(2) &= 0 \\
d(\sqrt{2n}) &=  \frac{|nD+2D-\sqrt{2n}D-D|}{\sqrt{(B-\frac{D}{2n})^2+(-1)^2}} \\
d(n) &= 0.
\end{align*}
Since $d(\sqrt{2n}) > 0$,
$$k^*_{\mathrm{optimal}} = \sqrt{2n}$$
which fits the relationship of $k^*_{\mathrm{optimal}}$ vs. $n$ (Figure 9).

# Results and discussion

Interpretations of early literature have resulted in lasting misconceptions about the use of cross-validation. Such misconceptions include the idea that there is a bias-variance trade-off $Bias^2(\hat{\theta}) \propto 1/Var(\hat{\theta})$ associated with selection of $k$ and that $k = 10$ is the best value to use in $k$-fold cross-validation (@Kohavi2001).

In agreement with a small but growing body of literature, our simulation results suggest that neither of these ideas are necessarily correct. Instead, in the context of linear and LASSO regression with standard normal data and certain random error, we find that for various $n$ both bias and variance decrease as $k$ increases (Figures 1 - 6), i.e., $Bias^2(\hat{\theta}) \propto Var(\hat{\theta})$, and although in the case of LASSO, 10-fold cross-validation seems to be a near-optimal choice for large $n$, for smaller samples and linear regression other values of $k$ appear to be optimal for model selection (Figures 7 and 8).

As previous authors have noted (@Zhang2015), it is important to distinguish between possible goals of cross-validation that previously have been conflated (@Kohavi2001): estimation of $\mathrm{PE}$, in which case $k_\mathrm{optimal} = {\arg\min}_{k}(\mathrm{PE} - \mathrm{\widehat{PE}})$, and model selection, in which case $$k_\mathrm{optimal} = {\arg\max}_k(A)$$, which is related to the definition of $k_\mathrm{optimal}^*$ used in this study.

We find that in the cases of both linear and LASSO regression, as $n$ increases, $k_\mathrm{optimal}^*$ increases but at a lower rate than $n$, such that $k_\mathrm{optimal}^*/n$ decreases, perhaps asymptotically. For LASSO regression, we also find that as $k$ increases past a certain value, lowest-error model selection becomes less reliable, i.e., $\hat{A}$ decreases, i.e., the true model has the lowest $\mathrm{MSE}$ less frequently (Figure 8). The rate of this reduction increases with $n$.

The reason for this is related to what is known as the cross-validation paradox (@Yang2006): greater quantity of data results in more accurate estimation of $\mathrm{PE}$; occasionally, this makes model-wise differences in $\mathrm{\widehat{PE}} = CV_{(k)}$ less exaggerated, making it more difficult to distinguish between models and resulting in less accurate model selection.

However, we do not observe the cross-validation paradox in our linear regression simulations. Although cross-validation results in similar bias-variance reductions in both cases, in the case of linear regression $\hat{A}$ increases with increasing $k$ initially before leveling off. However, this is not surprising, as our linear regression simulation does not involve variable selection, so that the differences in models are more pronounced. To observe the cross-validation paradox in linear regression would require comparison of more similar linear models predisposed to similarity in $\widehat{\mathrm{PE}}$, as in the simulation of LASSO regression.

In the cases of both linear and LASSO regression, $k_\mathrm{optimal}^*/n$ seems to change predictably with $n$. Specifically, it may be possible to model the relationship with some asymptotically decreasing function, while the relationship between $k_\mathrm{optimal}^*$ and $n$ seems to follow a positive pattern (Figures 9 and 10).

# Conclusion

Early literature suggests that increasing cross-validation fold number is related to decreasing bias and increasing variance of error estimation. However, more recent work suggests that this is not the case. Instead, increasing $k$ results in bias and variance reduction. This phenomenon is observable in our simulation results, which suggest that bias and variance decrease asymptotically with increasing $k$.

Our results also indicate a predictable relationship between $k_\mathrm{optimal}^*$ and sample size $n$. Although further data and analysis are needed to draw any reliable conclusions, modeling the relationship between $n$ and $k_\mathrm{optimal}^*$ would have practical utility, potentially improving the selection of $k$ from a largely arbitrary decision between 5 and 10.

Future research may also focus on error estimation of other models, including models capturing non-linear relationships or involving tuning of multiple hyper-parameters, e.g., random forest or gradient boosting. It may also be interesting to study the value of $k$ in repeated cross-validation or nested cross-validation, with the value of $k$ variable in the inner loop, outer loop, or both.

# Appendix

```{r echo=FALSE, fig.height = 8, fig.cap = "Linear regression squared bias vs. fold number for various sample sizes. As fold number increases, bias decreases initially before leveling off."}
source("~/optimal_k_git/plots/plots_linear.R")
figure1
```

```{r echo=FALSE, fig.height = 8, fig.cap = "Linear regression variance vs. fold number for various sample sizes. As fold number increases, variance decreases initially before leveling off."}
figure2
```

```{r echo=FALSE, fig.height = 8, fig.cap = "Linear regression mean squared error (MSE) vs. fold number for various sample sizes. As fold number increases, MSE decreases initially before leveling off."}
figure3
```

```{r echo=FALSE, fig.cap = "LASSO regression squared bias vs. fold number for various sample sizes. As fold number increases, bias decreases initially before leveling off."}
source("~/optimal_k_git/plots/plots_lasso.R")
figure4
```

```{r echo=FALSE, fig.cap = "LASSO regression variance vs. fold number for various sample sizes. As fold number increases, variance decreases initially before leveling off."}
figure5
```

```{r echo=FALSE, fig.cap = "LASSO regression mean squared error (MSE) vs. fold number for various sample sizes. As fold number increases, MSE decreases initially before leveling off."}
figure6
```

```{r echo=FALSE, fig.height = 8, fig.cap = "Count of true linear regression model selection by the lowest-error criterion vs. fold number for various sample sizes. Red line indicates optimal value of k."}
figure7
```

```{r echo=FALSE, fig.cap = "Count of true LASSO regression model selection by the lowest-error criterion vs. fold number for various sample sizes. Red line indicates optimal value of k."}
figure8
```

```{r echo=FALSE, fig.cap = "Raw optimal fold number vs. sample size (top); fold number proportion of sample size vs. sample size (bottom)."}
figure9
```

# Acknowledgements {.unnumbered}

Dr. Gregory Matthews's Fall 2022 Predictive Analytics class.

# Supplementary Material {.unnumbered}

All code for reproducing the analyses in this paper is publicly available at <https://github.com/gjm112/optimal_k>.

# References


